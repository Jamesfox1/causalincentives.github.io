
# Papers
**[Agent Incentives: A Causal Perspective](https://arxiv.org/abs/2102.01685)** (AI:ACP): presents sound and complete graphical criteria for four incentive concepts: value of information, value of control, response incentives, and control incentives.  
*T. Everitt\*, R. Carey\*, E. Langlois\*, PA. Ortega, S. Legg*.  
AAAI-21

**[How RL Agents Behave When Their Actions Are Modified](https://arxiv.org/abs/2102.07716)**: studies how user interventions affect the learning of different RL algorithms.  
*E. Langlois, T. Everitt*.  
AAAI-21  

**[Equilibrium Refinements for Multi-Agent Influence Diagrams: Theory and Practice](https://arxiv.org/abs/2102.05008)** ([video](https://slideslive.com/38954945/equilibrium-refinements-for-multiagent-influence-diagrams-theory-and-practice)): introduces a notion of subgames in multi-agent (causal) influence diagrams, alongside classic equilibrium refinements. The paper also reports on [pycid](https://github.com/causalincentives/pycid).  
*L. Hammond, J. Fox, T. Everitt, A. Abate, M. Wooldridge*.  
AAMAS-21  

**[Reward tampering problems and solutions in reinforcement learning: A causal influence diagram perspective](https://arxiv.org/abs/1908.04734)** ([summary](https://medium.com/@deepmindsafetyresearch/designing-agent-incentives-to-avoid-reward-tampering-4380c1bb6cd)): analyzes various reward tampering (aka "wireheading") problems with causal influence diagrams.  
*T. Everitt, M. Hutter, R. Kumar, V. Krakovna*  
Synthese, 2021

**[Modeling AGI safety frameworks with causal influence diagrams](https://arxiv.org/abs/1906.08663)**  
*T. Everitt, R. Kumar, V. Krakovna, S. Legg*  
IJCAI AI Safety Workshop, 2019

**[The Incentives that Shape Behavior](https://arxiv.org/abs/2001.07118)** ([summary](https://towardsdatascience.com/new-paper-the-incentives-that-shape-behaviour-d6d8bb77d2e4)): superseded by AI:ACP.  
*R Carey\*, E Langlois\*, T Everitt, S Legg*

**[Understanding Agent Incentives using Causal Influence Diagrams. Part I: Single Action Settings](https://arxiv.org/abs/1902.09980)** ([summary](https://medium.com/@deepmindsafetyresearch/understanding-agent-incentives-with-causal-influence-diagrams-7262c2512486)): superseded by AI:ACP.  
*T. Everitt, P.A. Ortega, E. Barnes, S. Legg*

*(\* denotes equal contribution)*


# Software

**[pycid](https://github.com/causalincentives/pycid)**: A Python implementation of causal influence diagrams, built on [pgmpy](https://pgmpy.org/).

**[CID Latex Package](https://github.com/causalincentives/cid-latex)**: A package for drawing professional looking influence diagrams, see [tutorial](https://causalincentives.github.io/cid-latex/CausalInfluenceDiagramLatexTutorial.html).



# Working group members

* **[Tom Everitt](https://www.tomeveritt.se/)**: DeepMind
* **[Ryan Carey](https://www.fhi.ox.ac.uk/team/ryan-carey/)**: University of Oxford
* **[Eric D. Langlois](https://www.linkedin.com/in/edtsft/?ppe=1)**: University of Toronto
* **[Carolyn Ashurst](https://www.fhi.ox.ac.uk/team/carolyn-ashurst/)**: Alan Turing Institute
* **[Chris van Merwijk](https://www.fhi.ox.ac.uk/team/chris-van-merwijk/)**: Future of Humanity Institute, University of Oxford
* **[Lewis Hammond](http://www.cs.ox.ac.uk/people/lewis.hammond/)**: University of Oxford
* **[James Fox](http://www.cs.ox.ac.uk/people/james.fox/)**: University of Oxford
* **[Zac Kenton](https://zackenton.github.io/)**: DeepMind
* **[Ramana Kumar](https://scholar.google.co.uk/citations?user=OyX1-qYAAAAJ&hl=en)**: DeepMind
* **[Sebastian Farquhar](https://sebastianfarquhar.com/)**: University of Oxford, DeepMind
